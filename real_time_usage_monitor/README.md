
# Realâ€‘Time Product Usage Monitor

Endâ€‘toâ€‘end showcase of a streaming analytics stack:

* **Kafka â†’ Spark Structured Streaming â†’ Delta Lake**
* **AI anomaly detection** (scikitâ€‘learn IsolationÂ Forest)
* **Live dashboard** (Streamlit)
* **dbt models** + dataâ€‘quality tests
* CI pipeline with pytest & GitHubÂ Actions

> **Note**: all data is synthetic, generated on the fly by `data_simulator/`.

---

## ğŸ“ Architecture

```mermaid
graph TD
    A[Data simulator<br>(Python + Faker)] -->|JSON events| K(Kafka topic)
    K --> S(Spark Structured Streaming)
    S --> D[Delta Lake<br>silver & gold]
    D --> B[dbt models]
    D --> ST[Streamlit dashboard]
    B -->|tests| CI(CI GitHub Actions)
```

---

## ğŸš€ QuickÂ start (local demo)

```bash
# 1) spin up Kafka + Spark (docker compose)
docker compose up -d

# 2) start event generator
python data_simulator/simulate_events.py

# 3) run streaming job (in new terminal)
python spark_app/streaming_job.py

# 4) launch dashboard
streamlit run streamlit_app.py
```

---

## Repo layout

```
data_simulator/         # event generator (writes to Kafka or CSV fallback)
spark_app/              # PySpark Structured Streaming job
streamlit_app.py        # realâ€‘time KPI & anomaly feed
dbt_demo_project/       # minimal dbt skeleton
notebooks/
â””â”€â”€ anomaly_exploration.ipynb
tests/                  # pytest dataâ€‘quality checks
.github/workflows/ci.yml
requirements.txt
docker-compose.yml
```

---

## âš™ï¸ Tech versions

* PythonÂ 3.10
* KafkaÂ 3.x (Redpanda quickstart)
* Apache SparkÂ 3.4 (pyspark)
* DeltaÂ 2.x
