# Real-Time Product Usage Monitor
![CI](https://github.com/danymitkova/real-time-usage-monitor/actions/workflows/ci.yml/badge.svg?branch=main)

End-to-end, **streaming analytics** stack:

* **Kafka â†’ Spark Structured Streaming â†’ Delta Lake**
* **AI anomaly detection** (Isolation Forest)
* **Live dashboard** (Streamlit)
* **dbt models** + data-quality tests
* **CI** pipeline with pytest & GitHub Actions

> **Data is synthetic** â€“ generated by `data_simulator/`.

---

## ğŸ“ Architecture

```mermaid
graph TD
    A["Data simulator (Faker)"] -->|JSON events| K[Kafka topic]
    K --> S[Spark Structured Streaming]
    S --> D[Delta Lake]
    D --> B[dbt models]
    D --> ST[Streamlit dashboard]
    B -->|tests| CI[GitHub Actions]


ğŸš€ Quick start (local demo)
bash
Copy
Edit
# clone & enter the project
git clone https://github.com/<YOUR_GH_USER>/real-time-usage-monitor.git
cd real-time-usage-monitor

# 1) spin up Kafka + Spark
docker compose up -d

# 2) start event generator (writes to Kafka, or stdout if broker missing)
python data_simulator/simulate_events.py --kafka

# 3) run streaming job
python spark_app/streaming_job.py

# 4) launch real-time dashboard
streamlit run streamlit_app.py
Open http://localhost:8501 â†’ see KPIs updating every few seconds.

ğŸ—‚ï¸ Project layout
bash
Copy
Edit
data_simulator/         # event generator
spark_app/              # PySpark Structured Streaming job
streamlit_app.py        # live KPI & anomaly feed
dbt_demo_project/       # minimal dbt skeleton
notebooks/
â””â”€â”€ anomaly_exploration.ipynb
tests/                  # pytest checks
docker-compose.yml      # Redpanda Kafka + Spark
requirements.txt
.github/workflows/ci.yml
âš™ï¸ Tech versions
Component	Version
Python	3.10
Kafka	Redpanda 23.2
Spark	3.4
Delta	2.x
Streamlit	1.35

ğŸ—ï¸ How it works
simulate_events.py emits JSON events at ~10 msg/s to Kafka topic events.

streaming_job.py consumes the topic with PySpark, aggregates 1-min windows
and writes results to Delta Lake (/tmp/delta/usage_agg).

Streamlit dashboard polls the Delta table every 5 s, showing:

Events per minute (stacked line)

Latest total events

Highlighted anomalies (Isolation Forest score < threshold).

dbt folder outlines where staging & mart models would live.

CI installs deps and runs basic data-quality tests on sample CSV.

ğŸ§ª Run tests locally
bash
Copy
Edit
pip install -r requirements.txt
pytest -q
ğŸ“ˆ Extending
Swap Redpanda with full Confluent Kafka & use readStream.format("kafka").

Add Superset or Power BI Direct Lake as BI layer.

Deploy Streamlit app to Streamlit Community Cloud and share the link.

Configure Unity Catalog permissions on Delta tables.

