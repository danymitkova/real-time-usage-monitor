# Real-Time Product Usage Monitor
![CI](https://github.com/danymitkova/real-time-usage-monitor/actions/workflows/ci.yml/badge.svg?branch=main)

End-to-end **streaming analytics** stack:

* **Kafka â†’ Spark Structured Streaming â†’ Delta Lake**
* **AI anomaly detection** (Isolation Forest)
* **Live dashboard** (Streamlit)
* **dbt models** + data-quality tests  
* **CI** pipeline with pytest & GitHub Actions  

> **Data is synthetic** â€“ generated by `data_simulator/`.

---

## ğŸ“ Architecture

![Architecture diagram](.github/diagram.png)


ğŸš€ Quick start (local demo)
bash
Copy
Edit
# clone & enter the project
git clone https://github.com/danymitkova/real-time-usage-monitor.git
cd real-time-usage-monitor

# 1) spin up Kafka + Spark
docker compose up -d

# 2) start event generator (writes to Kafka)
python data_simulator/simulate_events.py --kafka

# 3) run streaming job
python spark_app/streaming_job.py

# 4) launch real-time dashboard
streamlit run streamlit_app.py
Open http://localhost:8501 â†’ KPIs update every few seconds.

ğŸ—‚ï¸ Project layout
bash
Copy
Edit
data_simulator/         # event generator
spark_app/              # PySpark Structured Streaming job
streamlit_app.py        # live KPI & anomaly feed
dbt_demo_project/       # minimal dbt skeleton
notebooks/
â””â”€â”€ anomaly_exploration.ipynb
tests/                  # pytest checks
docker-compose.yml      # Redpanda Kafka + Spark
requirements.txt
.github/workflows/ci.yml
âš™ï¸ Tech versions
Component	Version
Python	3.10
Kafka	Redpanda 23.2
Spark	3.4
Delta	2.x
Streamlit	1.35

ğŸ—ï¸ How it works
simulate_events.py emits JSON events (~10 msg/s) to Kafka topic events.

streaming_job.py consumes the topic with PySpark, aggregates 1-min windows, writes to Delta Lake.

Streamlit polls Delta every 5 s, showing live KPIs & anomalies.

dbt folder outlines staging & mart models.

CI installs deps and runs basic data-quality tests.

ğŸ§ª Run tests locally
bash
Copy
Edit
pip install -r requirements.txt
pytest -q
ğŸ“ˆ Extending
Switch to Confluent Kafka (readStream.format("kafka"))<br>

Add Superset or Power BI Direct Lake<br>

Deploy Streamlit app to Streamlit Community Cloud<br>

Configure Unity Catalog permissions on Delta tables.
